{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db3bed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"HF_TOKEN\"]=os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05b7e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "915f742c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'user',\n",
       " 'id': '6620f1f17537261dc33caafe',\n",
       " 'name': 'sachchida',\n",
       " 'fullname': 'sachchida nand singh',\n",
       " 'isPro': False,\n",
       " 'avatarUrl': '/avatars/bdfb8002511b065e227b2231f48b8f5c.svg',\n",
       " 'orgs': [],\n",
       " 'auth': {'type': 'access_token',\n",
       "  'accessToken': {'displayName': 'ai-pc',\n",
       "   'role': 'fineGrained',\n",
       "   'createdAt': '2025-10-12T06:51:18.257Z',\n",
       "   'fineGrained': {'canReadGatedRepos': True,\n",
       "    'global': ['discussion.write', 'post.write'],\n",
       "    'scoped': [{'entity': {'_id': '6620f1f17537261dc33caafe',\n",
       "       'type': 'user',\n",
       "       'name': 'sachchida'},\n",
       "      'permissions': ['repo.content.read',\n",
       "       'repo.write',\n",
       "       'inference.serverless.write',\n",
       "       'inference.endpoints.infer.write',\n",
       "       'inference.endpoints.write',\n",
       "       'user.webhooks.read',\n",
       "       'user.webhooks.write',\n",
       "       'collection.read',\n",
       "       'collection.write',\n",
       "       'discussion.write']}]}}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = whoami(token=os.getenv(\"HF_TOKEN\"))\n",
    "user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8269247b",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "409 Client Error: Conflict for url: https://huggingface.co/api/repos/create (Request ID: Root=1-68f6626d-2d68757f33518f2c11d02e35;77c92528-501d-4bc2-9640-4a09ef10955e)\n\nYou already created this model repo: sachchida/my-first-repo",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch-cuda12_8/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:407\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch-cuda12_8/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 409 Client Error: Conflict for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfApi\n\u001b[32m      2\u001b[39m api = HfApi()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmy-first-repo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch-cuda12_8/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch-cuda12_8/lib/python3.12/site-packages/huggingface_hub/hf_api.py:3766\u001b[39m, in \u001b[36mHfApi.create_repo\u001b[39m\u001b[34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[39m\n\u001b[32m   3763\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   3765\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3766\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3767\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   3768\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err.response.status_code == \u001b[32m409\u001b[39m:\n\u001b[32m   3769\u001b[39m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch-cuda12_8/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:480\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    478\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 409 Client Error: Conflict for url: https://huggingface.co/api/repos/create (Request ID: Root=1-68f6626d-2d68757f33518f2c11d02e35;77c92528-501d-4bc2-9640-4a09ef10955e)\n\nYou already created this model repo: sachchida/my-first-repo"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=\"my-first-repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08687f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/sachchida/my-first-private-repo', endpoint='https://huggingface.co', repo_type='model', repo_id='sachchida/my-first-private-repo')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.create_repo(repo_id=\"my-first-private-repo\", private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6428f56",
   "metadata": {},
   "source": [
    "### Download a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99b6378e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sachchida/.cache/huggingface/hub/models--lysandre--arxiv-nlp/snapshots/c7a2e68263d13db10671379c23cf2a8ea0e12789/config.json'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id=\"lysandre/arxiv-nlp\", filename=\"config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f04b0d",
   "metadata": {},
   "source": [
    "You can use the hf download command from the terminal to directly download files from the Hub. Internally, it uses the same hf_hub_download() and snapshot_download() helpers described above and prints the returned path to the terminal.\n",
    "\n",
    "Copied\n",
    ">>> hf download gpt2 config.json\n",
    ">>> hf download gpt2 config.json model.safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aa2cd8",
   "metadata": {},
   "source": [
    "Upload a file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1b3e94",
   "metadata": {},
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "api.upload_file(\n",
    "    path_or_fileobj=\"/path/to/local/folder/README.md\",\n",
    "    path_in_repo=\"README.md\",\n",
    "    repo_id=\"username/test-dataset\",\n",
    "    repo_type=\"dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef007d",
   "metadata": {},
   "source": [
    "### Upload a folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a4666",
   "metadata": {},
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "# Upload all the content from the local folder to your remote Space.\n",
    "# By default, files are uploaded at the root of the repo\n",
    "api.upload_folder(\n",
    "    folder_path=\"/path/to/local/space\",\n",
    "    repo_id=\"username/my-cool-space\",\n",
    "    repo_type=\"space\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7aad7d",
   "metadata": {},
   "source": [
    "Here is how to use upload_large_folder() in a script. The method signature is very similar to upload_folder():\n",
    "\n",
    "Copied\n",
    "api.upload_large_folder(\n",
    "    repo_id=\"HuggingFaceM4/Docmatix\",\n",
    "    repo_type=\"dataset\",\n",
    "    folder_path=\"/path/to/local/docmatix\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70610970",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "402 Client Error: Payment Required for url: https://api.replicate.com/v1/models/black-forest-labs/flux-schnell/predictions",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch-cuda12_8/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:407\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch-cuda12_8/lib/python3.12/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://api.replicate.com/v1/models/black-forest-labs/flux-schnell/predictions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Example with an external provider (e.g. replicate)\u001b[39;00m\n\u001b[32m      5\u001b[39m replicate_client = InferenceClient(\n\u001b[32m      6\u001b[39m     provider=\u001b[33m\"\u001b[39m\u001b[33mreplicate\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mREPLICATE_API_TOKEN\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m replicate_image = \u001b[43mreplicate_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext_to_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mA flying car crossing a futuristic cityscape.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mblack-forest-labs/FLUX.1-schnell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m replicate_image.save(\u001b[33m\"\u001b[39m\u001b[33mflying_car.png\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch-cuda12_8/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:2566\u001b[39m, in \u001b[36mInferenceClient.text_to_image\u001b[39m\u001b[34m(self, prompt, negative_prompt, height, width, num_inference_steps, guidance_scale, model, scheduler, seed, extra_body)\u001b[39m\n\u001b[32m   2549\u001b[39m provider_helper = get_provider_helper(\u001b[38;5;28mself\u001b[39m.provider, task=\u001b[33m\"\u001b[39m\u001b[33mtext-to-image\u001b[39m\u001b[33m\"\u001b[39m, model=model_id)\n\u001b[32m   2550\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m   2551\u001b[39m     inputs=prompt,\n\u001b[32m   2552\u001b[39m     parameters={\n\u001b[32m   (...)\u001b[39m\u001b[32m   2564\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m   2565\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2566\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2567\u001b[39m response = provider_helper.get_response(response)\n\u001b[32m   2568\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _bytes_to_image(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch-cuda12_8/lib/python3.12/site-packages/huggingface_hub/inference/_client.py:275\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    272\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n\u001b[32m    277\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/torch-cuda12_8/lib/python3.12/site-packages/huggingface_hub/utils/_http.py:480\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    478\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    479\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://api.replicate.com/v1/models/black-forest-labs/flux-schnell/predictions"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "\n",
    "# Example with an external provider (e.g. replicate)\n",
    "replicate_client = InferenceClient(\n",
    "    provider=\"replicate\",\n",
    "    api_key=os.getenv(\"REPLICATE_API_TOKEN\")\n",
    ")\n",
    "replicate_image = replicate_client.text_to_image(\n",
    "    \"A flying car crossing a futuristic cityscape.\",\n",
    "    model=\"black-forest-labs/FLUX.1-schnell\",\n",
    ")\n",
    "replicate_image.save(\"flying_car.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cuda12_8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
