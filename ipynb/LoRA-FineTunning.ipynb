{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bc84eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachchida/anaconda3/envs/cuda13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    GPT2Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3284fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "def print_model_size(model, name=\"model\"):\n",
    "    torch.save(model.state_dict(), f\"{name}.pt\")\n",
    "    size_mb = os.path.getsize(f\"{name}.pt\") / (1024 ** 2)\n",
    "    print(f\"{name} size on disk: {size_mb:.2f} MB\")\n",
    "    os.remove(f\"{name}.pt\")\n",
    "\n",
    "def human_readable(num):\n",
    "    for unit in [\"\", \"K\", \"M\", \"B\"]:\n",
    "        if num < 1000:\n",
    "            return f\"{num:.2f}{unit}\"\n",
    "        num /= 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5abc758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1845559/1845559 [00:00<00:00, 7107089.24 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['headline', 'url', 'publisher', 'date', 'stock'],\n",
      "    num_rows: 1845559\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"ashraq/financial-news\", split=\"train\")\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a7b5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>url</th>\n",
       "      <th>publisher</th>\n",
       "      <th>date</th>\n",
       "      <th>stock</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>948540</th>\n",
       "      <td>KMG Chemicals' (KMG) CEO Chris Fraser on Q1 20...</td>\n",
       "      <td>http://seekingalpha.com/article/3748906-kmg-ch...</td>\n",
       "      <td>Seeking Alpha</td>\n",
       "      <td>2015-12-10 00:00:00</td>\n",
       "      <td>KMG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769643</th>\n",
       "      <td>90 Champions, Contenders And Challengers Are D...</td>\n",
       "      <td>https://seekingalpha.com/article/4219769-90-ch...</td>\n",
       "      <td>Seeking Alpha</td>\n",
       "      <td>2018-11-08 00:00:00</td>\n",
       "      <td>HEP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195804</th>\n",
       "      <td>Pabrai Funds Portfolio Review: POT, BPO, BIP, ...</td>\n",
       "      <td>http://www.gurufocus.com/news/148936/pabrai-fu...</td>\n",
       "      <td>GuruFocus</td>\n",
       "      <td>2011-10-21 00:00:00</td>\n",
       "      <td>BIP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3553</th>\n",
       "      <td>Auto Stock Roundup: Toyota Stays Top Seller, F...</td>\n",
       "      <td>http://www.zacks.com/stock/news/161391/auto-st...</td>\n",
       "      <td>Zacks</td>\n",
       "      <td>2015-01-22 00:00:00</td>\n",
       "      <td>AAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1716112</th>\n",
       "      <td>Deutsche Bank raises VMware price target to 17...</td>\n",
       "      <td>https://seekingalpha.com/news/3326141-deutsche...</td>\n",
       "      <td>Seeking Alpha</td>\n",
       "      <td>2018-01-29 00:00:00</td>\n",
       "      <td>VMW</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  headline  \\\n",
       "948540   KMG Chemicals' (KMG) CEO Chris Fraser on Q1 20...   \n",
       "769643   90 Champions, Contenders And Challengers Are D...   \n",
       "195804   Pabrai Funds Portfolio Review: POT, BPO, BIP, ...   \n",
       "3553     Auto Stock Roundup: Toyota Stays Top Seller, F...   \n",
       "1716112  Deutsche Bank raises VMware price target to 17...   \n",
       "\n",
       "                                                       url      publisher  \\\n",
       "948540   http://seekingalpha.com/article/3748906-kmg-ch...  Seeking Alpha   \n",
       "769643   https://seekingalpha.com/article/4219769-90-ch...  Seeking Alpha   \n",
       "195804   http://www.gurufocus.com/news/148936/pabrai-fu...      GuruFocus   \n",
       "3553     http://www.zacks.com/stock/news/161391/auto-st...          Zacks   \n",
       "1716112  https://seekingalpha.com/news/3326141-deutsche...  Seeking Alpha   \n",
       "\n",
       "                        date stock  \n",
       "948540   2015-12-10 00:00:00   KMG  \n",
       "769643   2018-11-08 00:00:00   HEP  \n",
       "195804   2011-10-21 00:00:00   BIP  \n",
       "3553     2015-01-22 00:00:00   AAP  \n",
       "1716112  2018-01-29 00:00:00   VMW  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert small slice for visualization\n",
    "df = dataset.to_pandas().sample(5000, random_state=42)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "937c3adb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# convert small slice for visualization\n",
    "df = dataset.to_pandas().sample(5000, random_state=42)\n",
    "\n",
    "# distribution of word counts\n",
    "df[\"length\"] = df[\"headline\"].str.split().apply(len)\n",
    "\n",
    "plt.hist(df[\"length\"], bins=50, color=\"steelblue\")\n",
    "plt.title(\"Word Count Distribution in Financial News\")\n",
    "plt.xlabel(\"Words per Article\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab8b82a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf04e916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149e59ec",
   "metadata": {},
   "source": [
    "### Inspect GPT-2 (Before LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9160c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ BASE GPT-2\n",
      "Total parameters     : 124.44M\n",
      "Trainable parameters : 124.44M\n",
      "Trainable %          : 100.00%\n",
      "gpt2_base size on disk: 237.40 MB\n"
     ]
    }
   ],
   "source": [
    "total, trainable = count_parameters(model)\n",
    "\n",
    "print(\"ðŸ”¹ BASE GPT-2\")\n",
    "print(f\"Total parameters     : {human_readable(total)}\")\n",
    "print(f\"Trainable parameters : {human_readable(trainable)}\")\n",
    "print(f\"Trainable %          : {100 * trainable / total:.2f}%\")\n",
    "\n",
    "print_model_size(model, \"gpt2_base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df53a76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 811,008 || all params: 125,250,816 || trainable%: 0.6475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sachchida/anaconda3/envs/cuda13/lib/python3.13/site-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a4e4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1661003/1661003 [01:41<00:00, 16403.64 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184556/184556 [00:11<00:00, 15657.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"headline\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "train_tok = train_dataset.map(tokenize, batched=True)\n",
    "eval_tok = eval_dataset.map(tokenize, batched=True)\n",
    "\n",
    "train_tok = train_tok.remove_columns([\"url\", \"headline\",\"publisher\",\"date\",\"stock\"])\n",
    "eval_tok = eval_tok.remove_columns([\"url\", \"headline\",\"publisher\",\"date\",\"stock\"])\n",
    "\n",
    "train_tok.set_format(\"torch\")\n",
    "eval_tok.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87776b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../model/gpt2_finance_lora\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_steps=500,\n",
    "    logging_steps=200,\n",
    "    num_train_epochs=3,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "166d790a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2711' max='311439' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2711/311439 02:43 < 5:10:22, 16.58 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.818300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.735300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.676500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.604900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.531700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.528300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.422900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.397300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>4.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>4.398800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>4.366000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/transformers/trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/transformers/trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/accelerate/utils/operations.py:819\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/accelerate/utils/operations.py:807\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/peft/peft_model.py:1923\u001b[39m, in \u001b[36mPeftModelForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1921\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(**kwargs):\n\u001b[32m   1922\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1924\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1929\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1930\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1931\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1934\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1935\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1936\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:308\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:1068\u001b[39m, in \u001b[36mGPT2LMHeadModel.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m   1048\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[33;03minput_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\u001b[39;00m\n\u001b[32m   1050\u001b[39m \u001b[33;03m    `input_ids_length` = `sequence_length` if `past_key_values` is `None` else\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1064\u001b[39m \u001b[33;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1066\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1068\u001b[39m transformer_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1084\u001b[39m hidden_states = transformer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1086\u001b[39m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:925\u001b[39m, in \u001b[36mGPT2Model.forward\u001b[39m\u001b[34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    923\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m outputs = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_checkpointing\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    938\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:413\u001b[39m, in \u001b[36mGPT2Block.forward\u001b[39m\u001b[34m(self, hidden_states, past_key_values, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    411\u001b[39m residual = hidden_states\n\u001b[32m    412\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.ln_1(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m attn_output, self_attn_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    417\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    419\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# residual connection\u001b[39;00m\n\u001b[32m    424\u001b[39m hidden_states = attn_output + residual\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/cuda13/lib/python3.13/site-packages/transformers/models/gpt2/modeling_gpt2.py:269\u001b[39m, in \u001b[36mGPT2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, past_key_values, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    265\u001b[39m     attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    271\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    272\u001b[39m     hidden_states: Optional[\u001b[38;5;28mtuple\u001b[39m[torch.FloatTensor]],\n\u001b[32m    273\u001b[39m     past_key_values: Optional[Cache] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    274\u001b[39m     cache_position: Optional[torch.LongTensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    275\u001b[39m     attention_mask: Optional[torch.FloatTensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    276\u001b[39m     head_mask: Optional[torch.FloatTensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    277\u001b[39m     encoder_hidden_states: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    278\u001b[39m     encoder_attention_mask: Optional[torch.FloatTensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    279\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    280\u001b[39m     **kwargs,\n\u001b[32m    281\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[Union[torch.Tensor, \u001b[38;5;28mtuple\u001b[39m[torch.Tensor]], ...]:\n\u001b[32m    282\u001b[39m     is_cross_attention = encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    283\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60d1c27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ GPT-2 + LoRA\n",
      "Total parameters     : 125.25M\n",
      "Trainable parameters : 811.01K\n",
      "Trainable %          : 0.6475%\n",
      "gpt2_lora size on disk: 240.56 MB\n"
     ]
    }
   ],
   "source": [
    "total, trainable = count_parameters(model)\n",
    "\n",
    "print(\"ðŸ”¹ GPT-2 + LoRA\")\n",
    "print(f\"Total parameters     : {human_readable(total)}\")\n",
    "print(f\"Trainable parameters : {human_readable(trainable)}\")\n",
    "print(f\"Trainable %          : {100 * trainable / total:.4f}%\")\n",
    "\n",
    "print_model_size(model, \"gpt2_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddf458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../model/gpt2_finance_lora\")\n",
    "tokenizer.save_pretrained(\"../model/gpt2_finance_lora\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
