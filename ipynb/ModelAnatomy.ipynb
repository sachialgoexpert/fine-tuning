{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6df7e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel,GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b1d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model=GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer=GPT2Tokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6424506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00fcc017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers: 12\n",
      "Heads: 12\n",
      "Hidden size: 768\n",
      "FFN size: 3072\n"
     ]
    }
   ],
   "source": [
    "config=model.config\n",
    "\n",
    "print(\"Layers:\", config.n_layer)\n",
    "print(\"Heads:\", config.n_head)\n",
    "print(\"Hidden size:\", config.n_embd)\n",
    "print(\"FFN size:\", 4 * config.n_embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bcd6588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 124439808\n",
      "Bias parameters: 102144\n",
      "Bias percentage: 0.08208305818022477\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "bias_count = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    total += param.numel()\n",
    "    if \"bias\" in name:\n",
    "        bias_count += param.numel()\n",
    "\n",
    "print(\"Total parameters:\", total)\n",
    "print(\"Bias parameters:\", bias_count)\n",
    "print(\"Bias percentage:\", bias_count / total * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71104bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding 2\n",
      "layernorm 50\n",
      "attention 48\n",
      "ffn 48\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "param_groups = defaultdict(list)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"attn\" in name:\n",
    "        param_groups[\"attention\"].append((name, param))\n",
    "    elif \"mlp\" in name:\n",
    "        param_groups[\"ffn\"].append((name, param))\n",
    "    elif \"ln\" in name:\n",
    "        param_groups[\"layernorm\"].append((name, param))\n",
    "    elif \"embed\" in name or \"wte\" in name or \"wpe\" in name:\n",
    "        param_groups[\"embedding\"].append((name, param))\n",
    "    else:\n",
    "        param_groups[\"other\"].append((name, param))\n",
    "\n",
    "for k in param_groups:\n",
    "    print(k, len(param_groups[k]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72859060",
   "metadata": {},
   "source": [
    "### Dummy Training Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f9e568a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    }
   ],
   "source": [
    "text = \"The transformer architecture is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "loss = outputs.loss\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d2930c",
   "metadata": {},
   "source": [
    "### Identify Parameters NOT Contributing to Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5145cb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gradient at all: 0\n",
      "Near-zero gradient: 0\n"
     ]
    }
   ],
   "source": [
    "dead_params = []\n",
    "tiny_grad_params = []\n",
    "\n",
    "THRESH = 1e-6\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is None:\n",
    "        dead_params.append(name)\n",
    "    else:\n",
    "        gnorm = param.grad.norm().item()\n",
    "        if gnorm < THRESH:\n",
    "            tiny_grad_params.append((name, gnorm))\n",
    "\n",
    "print(\"No gradient at all:\", len(dead_params))\n",
    "print(\"Near-zero gradient:\", len(tiny_grad_params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22090c7d",
   "metadata": {},
   "source": [
    "### Attention Headâ€“Level Gradient Analysis (Very Important)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc0a612a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 head grad norms: tensor([0.5569, 0.3553, 0.3728, 0.3998, 0.3836, 0.4077, 0.4608, 0.4338, 0.6736,\n",
      "        0.4773, 0.3687, 0.3877], device='cuda:0')\n",
      "Layer 1 head grad norms: tensor([0.1802, 0.2405, 0.1527, 0.1698, 0.1875, 0.3017, 0.3475, 0.4338, 0.3099,\n",
      "        0.1980, 0.1850, 0.3803], device='cuda:0')\n",
      "Layer 2 head grad norms: tensor([0.1841, 0.5805, 0.1350, 0.1870, 0.1795, 0.2435, 0.2480, 0.2832, 0.1293,\n",
      "        0.2410, 0.2017, 0.1819], device='cuda:0')\n",
      "Layer 3 head grad norms: tensor([0.1662, 0.1242, 0.3059, 0.1461, 0.1999, 0.2719, 0.2028, 0.1585, 0.2294,\n",
      "        0.1064, 0.2707, 0.1696], device='cuda:0')\n",
      "Layer 4 head grad norms: tensor([0.1371, 0.1102, 0.1738, 0.2548, 0.3914, 0.1502, 0.2171, 0.1568, 0.3115,\n",
      "        0.1308, 0.2506, 0.1038], device='cuda:0')\n",
      "Layer 5 head grad norms: tensor([0.2028, 0.1640, 0.1957, 0.2187, 0.2672, 0.2162, 0.1278, 0.1977, 0.1511,\n",
      "        0.2598, 0.3465, 0.3506], device='cuda:0')\n",
      "Layer 6 head grad norms: tensor([0.1687, 0.1632, 0.2366, 0.2068, 0.3035, 0.2748, 0.4246, 0.2187, 0.1671,\n",
      "        0.1919, 0.2874, 0.1582], device='cuda:0')\n",
      "Layer 7 head grad norms: tensor([0.1551, 0.2843, 0.1845, 0.1741, 0.1641, 0.1921, 0.3251, 0.2289, 0.1873,\n",
      "        0.1907, 0.1281, 0.1523], device='cuda:0')\n",
      "Layer 8 head grad norms: tensor([0.2122, 0.1809, 0.2585, 0.2396, 0.1644, 0.1962, 0.2480, 0.2412, 0.3741,\n",
      "        0.3605, 0.3138, 0.2164], device='cuda:0')\n",
      "Layer 9 head grad norms: tensor([0.2359, 0.1655, 0.2443, 0.2182, 0.2258, 0.3452, 0.2606, 0.1764, 0.2111,\n",
      "        0.2267, 0.2264, 0.1628], device='cuda:0')\n",
      "Layer 10 head grad norms: tensor([0.3375, 0.1902, 0.1895, 0.1461, 0.1876, 0.1805, 0.2169, 0.4170, 0.2124,\n",
      "        0.1996, 0.1645, 0.2582], device='cuda:0')\n",
      "Layer 11 head grad norms: tensor([0.3782, 0.4393, 0.5130, 0.6631, 0.3697, 0.3269, 0.3644, 0.3437, 0.2346,\n",
      "        0.4463, 0.4870, 0.4030], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def analyze_attention_heads(layer_idx):\n",
    "    block = model.transformer.h[layer_idx]\n",
    "    Wqkv = block.attn.c_attn.weight  # [768, 3*768]\n",
    "    grad = Wqkv.grad\n",
    "\n",
    "    d_model = config.n_embd\n",
    "    n_heads = config.n_head\n",
    "    head_dim = d_model // n_heads\n",
    "\n",
    "    grad = grad.view(d_model, 3, n_heads, head_dim)\n",
    "\n",
    "    head_norms = grad.norm(dim=(0, 3))  # (3, n_heads)\n",
    "\n",
    "    return head_norms.mean(dim=0)  # average QKV\n",
    "\n",
    "for i in range(config.n_layer):\n",
    "    norms = analyze_attention_heads(i)\n",
    "    print(f\"Layer {i} head grad norms:\", norms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b298789e",
   "metadata": {},
   "source": [
    "### Detect Dead Neurons in FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af5456c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 dead FFN neurons: 0\n",
      "Layer 1 dead FFN neurons: 0\n",
      "Layer 2 dead FFN neurons: 0\n",
      "Layer 3 dead FFN neurons: 0\n",
      "Layer 4 dead FFN neurons: 0\n",
      "Layer 5 dead FFN neurons: 0\n",
      "Layer 6 dead FFN neurons: 0\n",
      "Layer 7 dead FFN neurons: 0\n",
      "Layer 8 dead FFN neurons: 0\n",
      "Layer 9 dead FFN neurons: 0\n",
      "Layer 10 dead FFN neurons: 0\n",
      "Layer 11 dead FFN neurons: 0\n"
     ]
    }
   ],
   "source": [
    "def analyze_ffn_neurons(layer_idx):\n",
    "    block = model.transformer.h[layer_idx]\n",
    "    W1 = block.mlp.c_fc.weight.grad  # [3072, 768]\n",
    "\n",
    "    neuron_norms = W1.norm(dim=1)  # per neuron\n",
    "    return neuron_norms\n",
    "\n",
    "dead_neurons = []\n",
    "\n",
    "for i in range(config.n_layer):\n",
    "    norms = analyze_ffn_neurons(i)\n",
    "    dead = (norms < 1e-6).sum().item()\n",
    "    print(f\"Layer {i} dead FFN neurons:\", dead)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c075f3",
   "metadata": {},
   "source": [
    "### Layer-Level Redundancy Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64ba7851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 delta norm: 123.80655670166016\n",
      "Layer 1 delta norm: 286.76934814453125\n",
      "Layer 2 delta norm: 843.882568359375\n",
      "Layer 3 delta norm: 217.49032592773438\n",
      "Layer 4 delta norm: 231.7343292236328\n",
      "Layer 5 delta norm: 331.9229736328125\n",
      "Layer 6 delta norm: 225.51446533203125\n",
      "Layer 7 delta norm: 197.25697326660156\n",
      "Layer 8 delta norm: 192.83938598632812\n",
      "Layer 9 delta norm: 284.3265075683594\n",
      "Layer 10 delta norm: 385.94677734375\n",
      "Layer 11 delta norm: 3905.130615234375\n"
     ]
    }
   ],
   "source": [
    "def layer_effectiveness(layer_idx, x):\n",
    "    block = model.transformer.h[layer_idx]\n",
    "    with torch.no_grad():\n",
    "        y = block(x)[0]\n",
    "    return (y - x).norm().item()\n",
    "\n",
    "x = model.transformer.wte(inputs[\"input_ids\"]).to(device)\n",
    "\n",
    "for i in range(config.n_layer):\n",
    "    print(f\"Layer {i} delta norm:\", layer_effectiveness(i, x))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
